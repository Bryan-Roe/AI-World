{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5f2cdc",
   "metadata": {},
   "source": [
    "# AI Controller Development\n",
    "\n",
    "This notebook develops and tests an AI controller for autonomous agent decision-making in the 3D world environment.\n",
    "\n",
    "## Overview\n",
    "- Train intelligent agents using reinforcement learning\n",
    "- Integrate with Three.js world and LLM system\n",
    "- Test decision-making algorithms\n",
    "- Visualize agent behavior and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0746433b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Utilities\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)  # type: ignore\n",
    "random.seed(SEED)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16226cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (9,)\n",
      "State: [0.5        0.5        0.88       0.56       1.         1.\n",
      " 1.         0.17204651 1.        ]\n",
      "Action space size: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class AgentEnvironment:\n",
    "    \"\"\"\n",
    "    Simulated environment for training AI controller\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_size: int = 50):\n",
    "        self.grid_size = grid_size\n",
    "        self.agent_pos = np.array([grid_size // 2, grid_size // 2], dtype=float)\n",
    "        self.goal_pos = None\n",
    "        self.obstacles: list[np.ndarray] = []\n",
    "        self.threats: list[np.ndarray] = []\n",
    "        self.agent_health = 100\n",
    "        self.agent_energy = 100\n",
    "        self.steps = 0\n",
    "        self.max_steps = 200\n",
    "        \n",
    "        # State space: [pos_x, pos_y, goal_x, goal_y, health, energy, \n",
    "        #                threat_dist, obstacle_dist, steps_remaining]\n",
    "        self.state_size = 9\n",
    "        \n",
    "        # Action space: 8 discrete actions\n",
    "        self.action_size = 8\n",
    "        self.actions = {\n",
    "            0: 'move_up',\n",
    "            1: 'move_down',\n",
    "            2: 'move_left',\n",
    "            3: 'move_right',\n",
    "            4: 'move_toward_goal',\n",
    "            5: 'flee_threat',\n",
    "            6: 'rest',\n",
    "            7: 'explore'\n",
    "        }\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        self.agent_pos = np.array([self.grid_size // 2, self.grid_size // 2], dtype=float)\n",
    "        self.goal_pos = np.array([\n",
    "            np.random.randint(5, self.grid_size - 5),\n",
    "            np.random.randint(5, self.grid_size - 5)\n",
    "        ], dtype=float)\n",
    "        \n",
    "        # Random obstacles\n",
    "        self.obstacles = [\n",
    "            np.array([np.random.randint(0, self.grid_size), \n",
    "                     np.random.randint(0, self.grid_size)])\n",
    "            for _ in range(5)\n",
    "        ]\n",
    "        \n",
    "        # Random threats\n",
    "        self.threats = []\n",
    "        if np.random.random() < 0.3:\n",
    "            self.threats.append(\n",
    "                np.array([np.random.randint(0, self.grid_size),\n",
    "                         np.random.randint(0, self.grid_size)])\n",
    "            )\n",
    "        \n",
    "        self.agent_health = 100\n",
    "        self.agent_energy = 100\n",
    "        self.steps = 0\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Get current state vector\"\"\"\n",
    "        # Distance to goal\n",
    "        goal_dist = np.linalg.norm(self.agent_pos - self.goal_pos)\n",
    "        \n",
    "        # Distance to nearest threat\n",
    "        threat_dist = 50\n",
    "        if self.threats:\n",
    "            threat_dist = min([np.linalg.norm(self.agent_pos - t) for t in self.threats])\n",
    "        \n",
    "        # Distance to nearest obstacle\n",
    "        obstacle_dist = 50\n",
    "        if self.obstacles:\n",
    "            obstacle_dist = min([np.linalg.norm(self.agent_pos - o) for o in self.obstacles])\n",
    "        \n",
    "        state = np.array([\n",
    "            self.agent_pos[0] / self.grid_size,\n",
    "            self.agent_pos[1] / self.grid_size,\n",
    "            self.goal_pos[0] / self.grid_size,\n",
    "            self.goal_pos[1] / self.grid_size,\n",
    "            self.agent_health / 100,\n",
    "            self.agent_energy / 100,\n",
    "            min(threat_dist / 50, 1.0),\n",
    "            min(obstacle_dist / 50, 1.0),\n",
    "            (self.max_steps - self.steps) / self.max_steps\n",
    "        ])\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute action and return next state, reward, done\"\"\"\n",
    "        self.steps += 1\n",
    "        reward = -0.1  # Small penalty per step\n",
    "        \n",
    "        # Execute action\n",
    "        if action == 0:  # move_up\n",
    "            self.agent_pos[1] = min(self.agent_pos[1] + 1, self.grid_size - 1)\n",
    "            self.agent_energy -= 0.5\n",
    "        elif action == 1:  # move_down\n",
    "            self.agent_pos[1] = max(self.agent_pos[1] - 1, 0)\n",
    "            self.agent_energy -= 0.5\n",
    "        elif action == 2:  # move_left\n",
    "            self.agent_pos[0] = max(self.agent_pos[0] - 1, 0)\n",
    "            self.agent_energy -= 0.5\n",
    "        elif action == 3:  # move_right\n",
    "            self.agent_pos[0] = min(self.agent_pos[0] + 1, self.grid_size - 1)\n",
    "            self.agent_energy -= 0.5\n",
    "        elif action == 4:  # move_toward_goal\n",
    "            direction = self.goal_pos - self.agent_pos\n",
    "            if np.linalg.norm(direction) > 0:\n",
    "                direction = direction / np.linalg.norm(direction)\n",
    "                self.agent_pos += direction * 2\n",
    "                self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size - 1)\n",
    "            self.agent_energy -= 1\n",
    "            reward += 0.5  # Reward for moving toward goal\n",
    "        elif action == 5:  # flee_threat\n",
    "            if self.threats:\n",
    "                nearest_threat = min(self.threats, key=lambda t: np.linalg.norm(self.agent_pos - t))\n",
    "                direction = self.agent_pos - nearest_threat\n",
    "                if np.linalg.norm(direction) > 0:\n",
    "                    direction = direction / np.linalg.norm(direction)\n",
    "                    self.agent_pos += direction * 2\n",
    "                    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size - 1)\n",
    "            self.agent_energy -= 1.5\n",
    "        elif action == 6:  # rest\n",
    "            self.agent_energy = min(100, self.agent_energy + 10)\n",
    "            reward += 0.2\n",
    "        elif action == 7:  # explore\n",
    "            self.agent_pos += np.random.randn(2) * 2\n",
    "            self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size - 1)\n",
    "            self.agent_energy -= 0.3\n",
    "        \n",
    "        # Check threats\n",
    "        for threat in self.threats:\n",
    "            if np.linalg.norm(self.agent_pos - threat) < 3:\n",
    "                self.agent_health -= 10\n",
    "                reward -= 5\n",
    "        \n",
    "        # Check obstacles\n",
    "        for obstacle in self.obstacles:\n",
    "            if np.linalg.norm(self.agent_pos - obstacle) < 1:\n",
    "                reward -= 1\n",
    "        \n",
    "        # Check goal\n",
    "        goal_dist = np.linalg.norm(self.agent_pos - self.goal_pos)\n",
    "        if goal_dist < 2:\n",
    "            reward += 100\n",
    "            done = True\n",
    "        else:\n",
    "            reward += (50 - goal_dist) * 0.1  # Reward for getting closer\n",
    "            done = False\n",
    "        \n",
    "        # Check termination conditions\n",
    "        if self.agent_health <= 0 or self.agent_energy <= 0:\n",
    "            reward -= 50\n",
    "            done = True\n",
    "        elif self.steps >= self.max_steps:\n",
    "            done = True\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Visualize the environment\"\"\"\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        \n",
    "        # Grid\n",
    "        plt.xlim(0, self.grid_size)\n",
    "        plt.ylim(0, self.grid_size)\n",
    "        \n",
    "        # Obstacles\n",
    "        for obs in self.obstacles:\n",
    "            plt.plot(obs[0], obs[1], 'ks', markersize=10, label='Obstacle')\n",
    "        \n",
    "        # Threats\n",
    "        for threat in self.threats:\n",
    "            plt.plot(threat[0], threat[1], 'rs', markersize=15, label='Threat')\n",
    "        \n",
    "        # Goal\n",
    "        plt.plot(self.goal_pos[0], self.goal_pos[1], 'g*', markersize=20, label='Goal')\n",
    "        \n",
    "        # Agent\n",
    "        plt.plot(self.agent_pos[0], self.agent_pos[1], 'bo', markersize=15, label='Agent')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.title(f'Environment (Step {self.steps}, Health: {self.agent_health:.0f}, Energy: {self.agent_energy:.0f})')\n",
    "        plt.show()\n",
    "\n",
    "# Test the environment\n",
    "env = AgentEnvironment()\n",
    "state = env.reset()\n",
    "print(f\"State shape: {state.shape}\")\n",
    "print(f\"State: {state}\")\n",
    "print(f\"Action space size: {env.action_size}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50462d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controller initialized\n",
      "Policy network: DQNetwork(\n",
      "  (fc1): Linear(in_features=9, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n",
      "Starting epsilon: 1.0\n"
     ]
    }
   ],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network for action-value estimation\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for stable training\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(dones)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class AIController:\n",
    "    \"\"\"AI Controller using DQN\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, config=None):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.config = config or {\n",
    "            'learning_rate': 0.001,\n",
    "            'gamma': 0.99,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.01,\n",
    "            'epsilon_decay': 0.995,\n",
    "            'batch_size': 64,\n",
    "            'target_update': 10\n",
    "        }\n",
    "        \n",
    "        # Networks\n",
    "        self.policy_net = DQNetwork(state_size, action_size).to(device)\n",
    "        self.target_net = DQNetwork(state_size, action_size).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.policy_net.parameters(),\n",
    "            lr=self.config['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.memory = ReplayBuffer(capacity=10000)\n",
    "        \n",
    "        # Training state\n",
    "        self.epsilon = self.config['epsilon_start']\n",
    "        self.episode = 0\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        # Metrics\n",
    "        self.training_rewards = []\n",
    "        self.training_losses = []\n",
    "        \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                q_values = self.policy_net(state_tensor)\n",
    "                return q_values.argmax().item()\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one training step\"\"\"\n",
    "        if len(self.memory) < self.config['batch_size']:\n",
    "            return None\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(\n",
    "            self.config['batch_size']\n",
    "        )\n",
    "        \n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        next_states = next_states.to(device)\n",
    "        dones = dones.to(device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_net(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.config['gamma'] * next_q\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q.squeeze(), target_q)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy policy network weights to target network\"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.epsilon = max(\n",
    "            self.config['epsilon_end'],\n",
    "            self.epsilon * self.config['epsilon_decay']\n",
    "        )\n",
    "\n",
    "# Initialize controller\n",
    "controller = AIController(env.state_size, env.action_size)\n",
    "print(f\"Controller initialized\")\n",
    "print(f\"Policy network: {controller.policy_net}\")\n",
    "print(f\"Starting epsilon: {controller.epsilon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63f59d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function defined. Ready to train!\n"
     ]
    }
   ],
   "source": [
    "def train_agent(controller, env, num_episodes=100, verbose=True):\n",
    "    \"\"\"\n",
    "    Train the AI controller in the environment\n",
    "    \n",
    "    Args:\n",
    "        controller: AIController instance\n",
    "        env: AgentEnvironment instance\n",
    "        num_episodes: Number of training episodes\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    episode_losses = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = controller.select_action(state, training=True)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Store transition\n",
    "            controller.memory.push(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train\n",
    "            loss = controller.train_step()\n",
    "            if loss is not None:\n",
    "                episode_loss.append(loss)\n",
    "            \n",
    "            state = next_state\n",
    "            controller.total_steps += 1\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if episode % controller.config['target_update'] == 0:\n",
    "            controller.update_target_network()\n",
    "        \n",
    "        # Decay epsilon\n",
    "        controller.decay_epsilon()\n",
    "        controller.episode += 1\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(env.steps)\n",
    "        avg_loss = np.mean(episode_loss) if episode_loss else 0\n",
    "        episode_losses.append(avg_loss)\n",
    "        \n",
    "        controller.training_rewards.append(episode_reward)\n",
    "        controller.training_losses.append(avg_loss)\n",
    "        \n",
    "        if verbose and (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_length = np.mean(episode_lengths[-10:])\n",
    "            avg_loss_10 = np.mean(episode_losses[-10:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Avg Length: {avg_length:.1f} | \"\n",
    "                  f\"Loss: {avg_loss_10:.4f} | \"\n",
    "                  f\": {controller.epsilon:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'lengths': episode_lengths,\n",
    "        'losses': episode_losses\n",
    "    }\n",
    "\n",
    "print(\"Training function defined. Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6131a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training for 200 episodes\n",
      "\n",
      "Episode 10/200 | Avg Reward: 317.38 | Avg Length: 62.4 | Loss: 75.1060 | : 0.951\n",
      "Episode 20/200 | Avg Reward: 218.67 | Avg Length: 32.2 | Loss: 133.3205 | : 0.905\n",
      "Episode 30/200 | Avg Reward: 256.81 | Avg Length: 41.6 | Loss: 125.6797 | : 0.860\n",
      "Episode 40/200 | Avg Reward: 207.42 | Avg Length: 27.2 | Loss: 85.6892 | : 0.818\n",
      "Episode 50/200 | Avg Reward: 210.27 | Avg Length: 28.0 | Loss: 76.9359 | : 0.778\n",
      "Episode 60/200 | Avg Reward: 190.71 | Avg Length: 23.2 | Loss: 63.3397 | : 0.740\n",
      "Episode 70/200 | Avg Reward: 161.28 | Avg Length: 15.3 | Loss: 56.7911 | : 0.704\n",
      "Episode 80/200 | Avg Reward: 171.74 | Avg Length: 18.4 | Loss: 36.2498 | : 0.670\n",
      "Episode 90/200 | Avg Reward: 171.45 | Avg Length: 17.6 | Loss: 23.7087 | : 0.637\n",
      "Episode 100/200 | Avg Reward: 167.17 | Avg Length: 17.7 | Loss: 17.1993 | : 0.606\n",
      "Episode 110/200 | Avg Reward: 157.76 | Avg Length: 14.7 | Loss: 9.4339 | : 0.576\n",
      "Episode 120/200 | Avg Reward: 175.30 | Avg Length: 19.6 | Loss: 5.2172 | : 0.548\n",
      "Episode 130/200 | Avg Reward: 159.63 | Avg Length: 15.6 | Loss: 5.0160 | : 0.521\n",
      "Episode 140/200 | Avg Reward: 236.99 | Avg Length: 40.0 | Loss: 7.4830 | : 0.496\n",
      "Episode 150/200 | Avg Reward: 342.98 | Avg Length: 81.7 | Loss: 12.8841 | : 0.471\n",
      "Episode 160/200 | Avg Reward: 289.83 | Avg Length: 57.9 | Loss: 16.5407 | : 0.448\n"
     ]
    }
   ],
   "source": [
    "# Train the controller\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training for 200 episodes\\n\")\n",
    "\n",
    "training_history = train_agent(controller, env, num_episodes=200, verbose=True)\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Final epsilon: {controller.epsilon:.3f}\")\n",
    "print(f\"Total steps: {controller.total_steps}\")\n",
    "print(f\"Final 10 episode average reward: {np.mean(training_history['rewards'][-10:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35fc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(controller, env, num_episodes=10, render_last=True):\n",
    "    \"\"\"\n",
    "    Test the trained agent\n",
    "    \n",
    "    Args:\n",
    "        controller: Trained AIController\n",
    "        env: AgentEnvironment\n",
    "        num_episodes: Number of test episodes\n",
    "        render_last: Whether to render the last episode\n",
    "    \n",
    "    Returns:\n",
    "        Test results\n",
    "    \"\"\"\n",
    "    test_rewards = []\n",
    "    test_lengths = []\n",
    "    success_count = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action (no exploration)\n",
    "            action = controller.select_action(state, training=False)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        test_rewards.append(episode_reward)\n",
    "        test_lengths.append(env.steps)\n",
    "        \n",
    "        # Check if goal was reached\n",
    "        goal_dist = np.linalg.norm(env.agent_pos - env.goal_pos)\n",
    "        if goal_dist < 2:\n",
    "            success_count += 1\n",
    "        \n",
    "        print(f\"Test Episode {episode + 1}: \"\n",
    "              f\"Reward={episode_reward:.2f}, \"\n",
    "              f\"Steps={env.steps}, \"\n",
    "              f\"Goal Distance={goal_dist:.2f}\")\n",
    "        \n",
    "        # Render last episode\n",
    "        if render_last and episode == num_episodes - 1:\n",
    "            env.render()\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Average Reward: {np.mean(test_rewards):.2f}  {np.std(test_rewards):.2f}\")\n",
    "    print(f\"Average Steps: {np.mean(test_lengths):.1f}\")\n",
    "    print(f\"Success Rate: {success_count / num_episodes * 100:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': test_rewards,\n",
    "        'lengths': test_lengths,\n",
    "        'success_rate': success_count / num_episodes\n",
    "    }\n",
    "\n",
    "# Test the trained agent\n",
    "print(\"Testing trained agent...\\n\")\n",
    "test_results = test_agent(controller, env, num_episodes=10, render_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6bbb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Episode Rewards\n",
    "axes[0, 0].plot(training_history['rewards'], alpha=0.6, label='Raw')\n",
    "# Smooth with moving average\n",
    "window = 10\n",
    "if len(training_history['rewards']) >= window:\n",
    "    rewards_smooth = pd.Series(training_history['rewards']).rolling(window).mean()\n",
    "    axes[0, 0].plot(rewards_smooth, linewidth=2, label=f'{window}-episode MA')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].set_title('Training Rewards Over Time')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Episode Lengths\n",
    "axes[0, 1].plot(training_history['lengths'], alpha=0.6, label='Raw')\n",
    "if len(training_history['lengths']) >= window:\n",
    "    lengths_smooth = pd.Series(training_history['lengths']).rolling(window).mean()\n",
    "    axes[0, 1].plot(lengths_smooth, linewidth=2, label=f'{window}-episode MA')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Steps')\n",
    "axes[0, 1].set_title('Episode Length Over Time')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Training Loss\n",
    "axes[1, 0].plot(controller.training_losses, alpha=0.6, label='Raw')\n",
    "if len(controller.training_losses) >= window:\n",
    "    loss_smooth = pd.Series(controller.training_losses).rolling(window).mean()\n",
    "    axes[1, 0].plot(loss_smooth, linewidth=2, label=f'{window}-episode MA')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_title('Training Loss Over Time')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Reward Distribution\n",
    "axes[1, 1].hist(training_history['rewards'], bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].axvline(np.mean(training_history['rewards']), \n",
    "                   color='r', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[1, 1].set_xlabel('Episode Reward')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Reward Distribution')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n=== Training Summary ===\")\n",
    "print(f\"Total episodes: {len(training_history['rewards'])}\")\n",
    "print(f\"Total steps: {controller.total_steps}\")\n",
    "print(f\"\\nRewards:\")\n",
    "print(f\"  Mean: {np.mean(training_history['rewards']):.2f}\")\n",
    "print(f\"  Std: {np.std(training_history['rewards']):.2f}\")\n",
    "print(f\"  Max: {np.max(training_history['rewards']):.2f}\")\n",
    "print(f\"  Min: {np.min(training_history['rewards']):.2f}\")\n",
    "print(f\"\\nLast 20 episodes:\")\n",
    "print(f\"  Avg reward: {np.mean(training_history['rewards'][-20:]):.2f}\")\n",
    "print(f\"  Avg length: {np.mean(training_history['lengths'][-20:]):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bfbbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Q-values for different states\n",
    "def visualize_q_values(controller, env, num_samples=5):\n",
    "    \"\"\"Visualize Q-values for sample states\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        state = env.reset()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = controller.policy_net(state_tensor).cpu().numpy()[0]\n",
    "        \n",
    "        # Plot Q-values\n",
    "        axes[i].bar(range(env.action_size), q_values)\n",
    "        axes[i].set_xlabel('Action')\n",
    "        axes[i].set_ylabel('Q-Value')\n",
    "        axes[i].set_title(f'Sample {i+1}\\\\n'\n",
    "                         f'H:{env.agent_health:.0f} E:{env.agent_energy:.0f}\\\\n'\n",
    "                         f'Threats:{len(env.threats)}')\n",
    "        axes[i].set_xticks(range(env.action_size))\n",
    "        axes[i].set_xticklabels([env.actions[a][:8] for a in range(env.action_size)], \n",
    "                                rotation=45, ha='right')\n",
    "        axes[i].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Highlight best action\n",
    "        best_action = q_values.argmax()\n",
    "        axes[i].patches[best_action].set_color('green')\n",
    "        axes[i].patches[best_action].set_alpha(0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Q-Value Analysis for Different States', y=1.02, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "visualize_q_values(controller, env, num_samples=5)\n",
    "\n",
    "# Action distribution analysis\n",
    "print(\"\\n=== Action Distribution During Testing ===\")\n",
    "action_counts = np.zeros(env.action_size)\n",
    "\n",
    "for _ in range(50):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = controller.select_action(state, training=False)\n",
    "        action_counts[action] += 1\n",
    "        next_state, reward, done = env.step(action)\n",
    "        state = next_state\n",
    "\n",
    "# Plot action distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(env.action_size), action_counts)\n",
    "plt.xlabel('Action', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Action Selection Frequency (50 Test Episodes)', fontsize=14)\n",
    "plt.xticks(range(env.action_size), \n",
    "          [env.actions[a] for a in range(env.action_size)], \n",
    "          rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Color bars\n",
    "colors = plt.cm.viridis(action_counts / action_counts.max())\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAction frequencies:\")\n",
    "for action_id, count in enumerate(action_counts):\n",
    "    percentage = count / action_counts.sum() * 100\n",
    "    print(f\"{env.actions[action_id]:20s}: {count:4.0f} ({percentage:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for models\n",
    "model_dir = Path('ai_training/controller/models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save PyTorch checkpoint\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "checkpoint_path = model_dir / f'controller_{timestamp}.pth'\n",
    "\n",
    "checkpoint = {\n",
    "    'episode': controller.episode,\n",
    "    'total_steps': controller.total_steps,\n",
    "    'policy_net_state': controller.policy_net.state_dict(),\n",
    "    'target_net_state': controller.target_net.state_dict(),\n",
    "    'optimizer_state': controller.optimizer.state_dict(),\n",
    "    'epsilon': controller.epsilon,\n",
    "    'config': controller.config,\n",
    "    'training_rewards': controller.training_rewards,\n",
    "    'training_losses': controller.training_losses,\n",
    "    'state_size': controller.state_size,\n",
    "    'action_size': controller.action_size\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\" Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "# Export weights as JSON for JavaScript\n",
    "weights_export = {}\n",
    "for name, param in controller.policy_net.state_dict().items():\n",
    "    weights_export[name] = param.cpu().numpy().tolist()\n",
    "\n",
    "export_data = {\n",
    "    'weights': weights_export,\n",
    "    'config': controller.config,\n",
    "    'state_size': controller.state_size,\n",
    "    'action_size': controller.action_size,\n",
    "    'actions': env.actions,\n",
    "    'training_stats': {\n",
    "        'episodes': controller.episode,\n",
    "        'total_steps': controller.total_steps,\n",
    "        'final_epsilon': controller.epsilon,\n",
    "        'avg_reward_last_20': float(np.mean(controller.training_rewards[-20:])),\n",
    "        'test_success_rate': test_results['success_rate']\n",
    "    }\n",
    "}\n",
    "\n",
    "json_path = model_dir / f'controller_weights_{timestamp}.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\" Weights exported: {json_path}\")\n",
    "\n",
    "# Save latest version\n",
    "latest_path = model_dir / 'controller_latest.pth'\n",
    "torch.save(checkpoint, latest_path)\n",
    "print(f\" Latest checkpoint: {latest_path}\")\n",
    "\n",
    "latest_json = model_dir / 'controller_latest.json'\n",
    "with open(latest_json, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "print(f\" Latest weights: {latest_json}\")\n",
    "\n",
    "print(\"\\n=== Model Export Complete ===\")\n",
    "print(f\"Files saved in: {model_dir.absolute()}\")\n",
    "print(f\"\\\\nTo use in JavaScript:\")\n",
    "print(f\"  1. Load weights from: {latest_json}\")\n",
    "print(f\"  2. Import AIController from public/AIController.js\")\n",
    "print(f\"  3. Initialize with loaded weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb72a3ca",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Integration with Three.js World\n",
    "1. Load [public/AIController.js](public/AIController.js) in your game\n",
    "2. Register agents using `controller.registerAgent()`\n",
    "3. Call `controller.start()` to begin autonomous behavior\n",
    "\n",
    "### Training Improvements\n",
    "- Collect real gameplay data from the 3D world\n",
    "- Fine-tune with actual agent interactions\n",
    "- Run [ai_controller_trainer.py](ai_controller_trainer.py) for advanced training\n",
    "\n",
    "### Further Development\n",
    "- Add more complex behaviors (cooperation, communication)\n",
    "- Implement multi-agent scenarios\n",
    "- Integrate with LLM for natural language reasoning\n",
    "- Add memory and learning from past experiences\n",
    "\n",
    "Run this notebook again anytime to retrain and improve the AI controller!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b26d5b8",
   "metadata": {},
   "source": [
    "## 9. Save and Export Model\n",
    "\n",
    "Save the trained controller for deployment in the web application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b81044c",
   "metadata": {},
   "source": [
    "## 8. Analyze Decision Making\n",
    "\n",
    "Visualize Q-values and action preferences to understand the agent's decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e44b27",
   "metadata": {},
   "source": [
    "## 7. Visualize Performance\n",
    "\n",
    "Create comprehensive visualizations of the AI controller's learning progress and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f82ea",
   "metadata": {},
   "source": [
    "## 6. Test the AI Controller\n",
    "\n",
    "Evaluate the trained controller's performance on test scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7442d",
   "metadata": {},
   "source": [
    "## 5. Train the AI Controller\n",
    "\n",
    "Train the agent for multiple episodes and observe learning progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06de85",
   "metadata": {},
   "source": [
    "## 4. Implement Decision Making Logic\n",
    "\n",
    "Implement the training loop that allows the agent to learn from experience through trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da10e16a",
   "metadata": {},
   "source": [
    "## 3. Create the AI Controller Class\n",
    "\n",
    "Define the Deep Q-Network (DQN) architecture and experience replay buffer for the AI controller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13839bc2",
   "metadata": {},
   "source": [
    "## 2. Define the Environment\n",
    "\n",
    "Set up the simulation environment where the AI agent will operate. This includes state space (agent observations) and action space (possible actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ce4b9",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import necessary libraries for reinforcement learning, neural networks, and data processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
