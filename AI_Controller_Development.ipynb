{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5f2cdc",
   "metadata": {},
   "source": [
    "# AI Controller Development\n",
    "\n",
    "This notebook develops and tests an AI controller for autonomous agent decision-making in the 3D world environment.\n",
    "\n",
    "## Overview\n",
    "- Train intelligent agents using reinforcement learning\n",
    "- Integrate with Three.js world and LLM system\n",
    "- Test decision-making algorithms\n",
    "- Visualize agent behavior and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0746433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Utilities\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16226cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentEnvironment:\n",
    "    \"\"\"\n",
    "    Simulated environment for training AI controller\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_size=50):\n",
    "        self.grid_size = grid_size\n",
    "        self.agent_pos = np.array([grid_size // 2, grid_size // 2], dtype=float)\n",
    "        self.goal_pos = None\n",
    "        self.obstacles = []\n",
    "        self.threats = []\n",
    "        self.agent_health = 100\n",
    "        self.agent_energy = 100\n",
    "        self.steps = 0\n",
    "        self.max_steps = 200\n",
    "        \n",
    "        # State space: [pos_x, pos_y, goal_x, goal_y, health, energy, \n",
    "        #                threat_dist, obstacle_dist, steps_remaining]\n",
    "        self.state_size = 9\n",
    "        \n",
    "        # Action space: 8 discrete actions\n",
    "        self.action_size = 8\n",
    "        self.actions = {\n",
    "            0: 'move_up',\n",
    "            1: 'move_down',\n",
    "            2: 'move_left',\n",
    "            3: 'move_right',\n",
    "            4: 'move_toward_goal',\n",
    "            5: 'flee_threat',\n",
    "            6: 'rest',\n",
    "            7: 'explore'\n",
    "        }\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        self.agent_pos = np.array([self.grid_size // 2, self.grid_size // 2], dtype=float)\n",
    "        self.goal_pos = np.array([\n",
    "            np.random.randint(5, self.grid_size - 5),\n",
    "            np.random.randint(5, self.grid_size - 5)\n",
    "        ], dtype=float)\n",
    "        \n",
    "        # Random obstacles\n",
    "        self.obstacles = [\n",
    "            np.array([np.random.randint(0, self.grid_size), \n",
    "                     np.random.randint(0, self.grid_size)])\n",
    "            for _ in range(5)\n",
    "        ]\n",
    "        \n",
    "        # Random threats\n",
    "        self.threats = []\n",
    "        if np.random.random() < 0.3:\n",
    "            self.threats.append(\n",
    "                np.array([np.random.randint(0, self.grid_size),\n",
    "                         np.random.randint(0, self.grid_size)])\n",
    "            )\n",
    "        \n",
    "        self.agent_health = 100\n",
    "        self.agent_energy = 100\n",
    "        self.steps = 0\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Get current state vector\"\"\"\n",
    "        # Distance to goal\n",
    "        goal_dist = np.linalg.norm(self.agent_pos - self.goal_pos)\n",
    "        \n",
    "        # Distance to nearest threat\n",
    "        threat_dist = 50\n",
    "        if self.threats:\n",
    "            threat_dist = min([np.linalg.norm(self.agent_pos - t) for t in self.threats])\n",
    "        \n",
    "        # Distance to nearest obstacle\n",
    "        obstacle_dist = 50\n",
    "        if self.obstacles:\n",
    "            obstacle_dist = min([np.linalg.norm(self.agent_pos - o) for o in self.obstacles])\n",
    "        \n",
    "        state = np.array([\n",
    "            self.agent_pos[0] / self.grid_size,\n",
    "            self.agent_pos[1] / self.grid_size,\n",
    "            self.goal_pos[0] / self.grid_size,\n",
    "            self.goal_pos[1] / self.grid_size,\n",
    "            self.agent_health / 100,\n",
    "            self.agent_energy / 100,\n",
    "            min(threat_dist / 50, 1.0),\n",
    "            min(obstacle_dist / 50, 1.0),\n",
    "            (self.max_steps - self.steps) / self.max_steps\n",
    "        ])\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute action and return next state, reward, done\"\"\"\n",
    "        self.steps += 1\n",
    "        reward = -0.1  # Small penalty per step\n",
    "        \n",
    "        # Execute action\n",
    "        if action == 0:  # move_up\n",
    "            self.agent_pos[1] = min(self.agent_pos[1] + 1, self.grid_size - 1)\n",
    "            self.agent_energy -= 0.5\n",
    "        elif action == 1:  # move_down\n",
    "            self.agent_pos[1] = max(self.agent_pos[1] - 1, 0)\n",
    "            self.agent_energy -= 0.5\n",
    "        elif action == 2:  # move_left\n",
    "            self.agent_pos[0] = max(self.agent_pos[0] - 1, 0)\n",
    "            self.agent_energy -= 0.5\n",
    "        elif action == 3:  # move_right\n",
    "            self.agent_pos[0] = min(self.agent_pos[0] + 1, self.grid_size - 1)\n",
    "            self.agent_energy -= 0.5\n",
    "        elif action == 4:  # move_toward_goal\n",
    "            direction = self.goal_pos - self.agent_pos\n",
    "            if np.linalg.norm(direction) > 0:\n",
    "                direction = direction / np.linalg.norm(direction)\n",
    "                self.agent_pos += direction * 2\n",
    "                self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size - 1)\n",
    "            self.agent_energy -= 1\n",
    "            reward += 0.5  # Reward for moving toward goal\n",
    "        elif action == 5:  # flee_threat\n",
    "            if self.threats:\n",
    "                nearest_threat = min(self.threats, key=lambda t: np.linalg.norm(self.agent_pos - t))\n",
    "                direction = self.agent_pos - nearest_threat\n",
    "                if np.linalg.norm(direction) > 0:\n",
    "                    direction = direction / np.linalg.norm(direction)\n",
    "                    self.agent_pos += direction * 2\n",
    "                    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size - 1)\n",
    "            self.agent_energy -= 1.5\n",
    "        elif action == 6:  # rest\n",
    "            self.agent_energy = min(100, self.agent_energy + 10)\n",
    "            reward += 0.2\n",
    "        elif action == 7:  # explore\n",
    "            self.agent_pos += np.random.randn(2) * 2\n",
    "            self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size - 1)\n",
    "            self.agent_energy -= 0.3\n",
    "        \n",
    "        # Check threats\n",
    "        for threat in self.threats:\n",
    "            if np.linalg.norm(self.agent_pos - threat) < 3:\n",
    "                self.agent_health -= 10\n",
    "                reward -= 5\n",
    "        \n",
    "        # Check obstacles\n",
    "        for obstacle in self.obstacles:\n",
    "            if np.linalg.norm(self.agent_pos - obstacle) < 1:\n",
    "                reward -= 1\n",
    "        \n",
    "        # Check goal\n",
    "        goal_dist = np.linalg.norm(self.agent_pos - self.goal_pos)\n",
    "        if goal_dist < 2:\n",
    "            reward += 100\n",
    "            done = True\n",
    "        else:\n",
    "            reward += (50 - goal_dist) * 0.1  # Reward for getting closer\n",
    "            done = False\n",
    "        \n",
    "        # Check termination conditions\n",
    "        if self.agent_health <= 0 or self.agent_energy <= 0:\n",
    "            reward -= 50\n",
    "            done = True\n",
    "        elif self.steps >= self.max_steps:\n",
    "            done = True\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Visualize the environment\"\"\"\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        \n",
    "        # Grid\n",
    "        plt.xlim(0, self.grid_size)\n",
    "        plt.ylim(0, self.grid_size)\n",
    "        \n",
    "        # Obstacles\n",
    "        for obs in self.obstacles:\n",
    "            plt.plot(obs[0], obs[1], 'ks', markersize=10, label='Obstacle')\n",
    "        \n",
    "        # Threats\n",
    "        for threat in self.threats:\n",
    "            plt.plot(threat[0], threat[1], 'rs', markersize=15, label='Threat')\n",
    "        \n",
    "        # Goal\n",
    "        plt.plot(self.goal_pos[0], self.goal_pos[1], 'g*', markersize=20, label='Goal')\n",
    "        \n",
    "        # Agent\n",
    "        plt.plot(self.agent_pos[0], self.agent_pos[1], 'bo', markersize=15, label='Agent')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.title(f'Environment (Step {self.steps}, Health: {self.agent_health:.0f}, Energy: {self.agent_energy:.0f})')\n",
    "        plt.show()\n",
    "\n",
    "# Test the environment\n",
    "env = AgentEnvironment()\n",
    "state = env.reset()\n",
    "print(f\"State shape: {state.shape}\")\n",
    "print(f\"State: {state}\")\n",
    "print(f\"Action space size: {env.action_size}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50462d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network for action-value estimation\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for stable training\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(dones)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class AIController:\n",
    "    \"\"\"AI Controller using DQN\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, config=None):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.config = config or {\n",
    "            'learning_rate': 0.001,\n",
    "            'gamma': 0.99,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.01,\n",
    "            'epsilon_decay': 0.995,\n",
    "            'batch_size': 64,\n",
    "            'target_update': 10\n",
    "        }\n",
    "        \n",
    "        # Networks\n",
    "        self.policy_net = DQNetwork(state_size, action_size).to(device)\n",
    "        self.target_net = DQNetwork(state_size, action_size).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.policy_net.parameters(),\n",
    "            lr=self.config['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.memory = ReplayBuffer(capacity=10000)\n",
    "        \n",
    "        # Training state\n",
    "        self.epsilon = self.config['epsilon_start']\n",
    "        self.episode = 0\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        # Metrics\n",
    "        self.training_rewards = []\n",
    "        self.training_losses = []\n",
    "        \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                q_values = self.policy_net(state_tensor)\n",
    "                return q_values.argmax().item()\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one training step\"\"\"\n",
    "        if len(self.memory) < self.config['batch_size']:\n",
    "            return None\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(\n",
    "            self.config['batch_size']\n",
    "        )\n",
    "        \n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        next_states = next_states.to(device)\n",
    "        dones = dones.to(device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_net(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.config['gamma'] * next_q\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q.squeeze(), target_q)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy policy network weights to target network\"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.epsilon = max(\n",
    "            self.config['epsilon_end'],\n",
    "            self.epsilon * self.config['epsilon_decay']\n",
    "        )\n",
    "\n",
    "# Initialize controller\n",
    "controller = AIController(env.state_size, env.action_size)\n",
    "print(f\"Controller initialized\")\n",
    "print(f\"Policy network: {controller.policy_net}\")\n",
    "print(f\"Starting epsilon: {controller.epsilon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f59d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(controller, env, num_episodes=100, verbose=True):\n",
    "    \"\"\"\n",
    "    Train the AI controller in the environment\n",
    "    \n",
    "    Args:\n",
    "        controller: AIController instance\n",
    "        env: AgentEnvironment instance\n",
    "        num_episodes: Number of training episodes\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    episode_losses = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = controller.select_action(state, training=True)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Store transition\n",
    "            controller.memory.push(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train\n",
    "            loss = controller.train_step()\n",
    "            if loss is not None:\n",
    "                episode_loss.append(loss)\n",
    "            \n",
    "            state = next_state\n",
    "            controller.total_steps += 1\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if episode % controller.config['target_update'] == 0:\n",
    "            controller.update_target_network()\n",
    "        \n",
    "        # Decay epsilon\n",
    "        controller.decay_epsilon()\n",
    "        controller.episode += 1\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(env.steps)\n",
    "        avg_loss = np.mean(episode_loss) if episode_loss else 0\n",
    "        episode_losses.append(avg_loss)\n",
    "        \n",
    "        controller.training_rewards.append(episode_reward)\n",
    "        controller.training_losses.append(avg_loss)\n",
    "        \n",
    "        if verbose and (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_length = np.mean(episode_lengths[-10:])\n",
    "            avg_loss_10 = np.mean(episode_losses[-10:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Avg Length: {avg_length:.1f} | \"\n",
    "                  f\"Loss: {avg_loss_10:.4f} | \"\n",
    "                  f\"ε: {controller.epsilon:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'lengths': episode_lengths,\n",
    "        'losses': episode_losses\n",
    "    }\n",
    "\n",
    "print(\"Training function defined. Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6131a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the controller\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training for 200 episodes\\n\")\n",
    "\n",
    "training_history = train_agent(controller, env, num_episodes=200, verbose=True)\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Final epsilon: {controller.epsilon:.3f}\")\n",
    "print(f\"Total steps: {controller.total_steps}\")\n",
    "print(f\"Final 10 episode average reward: {np.mean(training_history['rewards'][-10:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35fc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(controller, env, num_episodes=10, render_last=True):\n",
    "    \"\"\"\n",
    "    Test the trained agent\n",
    "    \n",
    "    Args:\n",
    "        controller: Trained AIController\n",
    "        env: AgentEnvironment\n",
    "        num_episodes: Number of test episodes\n",
    "        render_last: Whether to render the last episode\n",
    "    \n",
    "    Returns:\n",
    "        Test results\n",
    "    \"\"\"\n",
    "    test_rewards = []\n",
    "    test_lengths = []\n",
    "    success_count = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action (no exploration)\n",
    "            action = controller.select_action(state, training=False)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        test_rewards.append(episode_reward)\n",
    "        test_lengths.append(env.steps)\n",
    "        \n",
    "        # Check if goal was reached\n",
    "        goal_dist = np.linalg.norm(env.agent_pos - env.goal_pos)\n",
    "        if goal_dist < 2:\n",
    "            success_count += 1\n",
    "        \n",
    "        print(f\"Test Episode {episode + 1}: \"\n",
    "              f\"Reward={episode_reward:.2f}, \"\n",
    "              f\"Steps={env.steps}, \"\n",
    "              f\"Goal Distance={goal_dist:.2f}\")\n",
    "        \n",
    "        # Render last episode\n",
    "        if render_last and episode == num_episodes - 1:\n",
    "            env.render()\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Average Reward: {np.mean(test_rewards):.2f} ± {np.std(test_rewards):.2f}\")\n",
    "    print(f\"Average Steps: {np.mean(test_lengths):.1f}\")\n",
    "    print(f\"Success Rate: {success_count / num_episodes * 100:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': test_rewards,\n",
    "        'lengths': test_lengths,\n",
    "        'success_rate': success_count / num_episodes\n",
    "    }\n",
    "\n",
    "# Test the trained agent\n",
    "print(\"Testing trained agent...\\n\")\n",
    "test_results = test_agent(controller, env, num_episodes=10, render_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6bbb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Episode Rewards\n",
    "axes[0, 0].plot(training_history['rewards'], alpha=0.6, label='Raw')\n",
    "# Smooth with moving average\n",
    "window = 10\n",
    "if len(training_history['rewards']) >= window:\n",
    "    rewards_smooth = pd.Series(training_history['rewards']).rolling(window).mean()\n",
    "    axes[0, 0].plot(rewards_smooth, linewidth=2, label=f'{window}-episode MA')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].set_title('Training Rewards Over Time')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Episode Lengths\n",
    "axes[0, 1].plot(training_history['lengths'], alpha=0.6, label='Raw')\n",
    "if len(training_history['lengths']) >= window:\n",
    "    lengths_smooth = pd.Series(training_history['lengths']).rolling(window).mean()\n",
    "    axes[0, 1].plot(lengths_smooth, linewidth=2, label=f'{window}-episode MA')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Steps')\n",
    "axes[0, 1].set_title('Episode Length Over Time')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Training Loss\n",
    "axes[1, 0].plot(controller.training_losses, alpha=0.6, label='Raw')\n",
    "if len(controller.training_losses) >= window:\n",
    "    loss_smooth = pd.Series(controller.training_losses).rolling(window).mean()\n",
    "    axes[1, 0].plot(loss_smooth, linewidth=2, label=f'{window}-episode MA')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_title('Training Loss Over Time')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Reward Distribution\n",
    "axes[1, 1].hist(training_history['rewards'], bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].axvline(np.mean(training_history['rewards']), \n",
    "                   color='r', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[1, 1].set_xlabel('Episode Reward')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Reward Distribution')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n=== Training Summary ===\")\n",
    "print(f\"Total episodes: {len(training_history['rewards'])}\")\n",
    "print(f\"Total steps: {controller.total_steps}\")\n",
    "print(f\"\\nRewards:\")\n",
    "print(f\"  Mean: {np.mean(training_history['rewards']):.2f}\")\n",
    "print(f\"  Std: {np.std(training_history['rewards']):.2f}\")\n",
    "print(f\"  Max: {np.max(training_history['rewards']):.2f}\")\n",
    "print(f\"  Min: {np.min(training_history['rewards']):.2f}\")\n",
    "print(f\"\\nLast 20 episodes:\")\n",
    "print(f\"  Avg reward: {np.mean(training_history['rewards'][-20:]):.2f}\")\n",
    "print(f\"  Avg length: {np.mean(training_history['lengths'][-20:]):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bfbbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Q-values for different states\n",
    "def visualize_q_values(controller, env, num_samples=5):\n",
    "    \"\"\"Visualize Q-values for sample states\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        state = env.reset()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = controller.policy_net(state_tensor).cpu().numpy()[0]\n",
    "        \n",
    "        # Plot Q-values\n",
    "        axes[i].bar(range(env.action_size), q_values)\n",
    "        axes[i].set_xlabel('Action')\n",
    "        axes[i].set_ylabel('Q-Value')\n",
    "        axes[i].set_title(f'Sample {i+1}\\\\n'\n",
    "                         f'H:{env.agent_health:.0f} E:{env.agent_energy:.0f}\\\\n'\n",
    "                         f'Threats:{len(env.threats)}')\n",
    "        axes[i].set_xticks(range(env.action_size))\n",
    "        axes[i].set_xticklabels([env.actions[a][:8] for a in range(env.action_size)], \n",
    "                                rotation=45, ha='right')\n",
    "        axes[i].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Highlight best action\n",
    "        best_action = q_values.argmax()\n",
    "        axes[i].patches[best_action].set_color('green')\n",
    "        axes[i].patches[best_action].set_alpha(0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Q-Value Analysis for Different States', y=1.02, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "visualize_q_values(controller, env, num_samples=5)\n",
    "\n",
    "# Action distribution analysis\n",
    "print(\"\\n=== Action Distribution During Testing ===\")\n",
    "action_counts = np.zeros(env.action_size)\n",
    "\n",
    "for _ in range(50):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = controller.select_action(state, training=False)\n",
    "        action_counts[action] += 1\n",
    "        next_state, reward, done = env.step(action)\n",
    "        state = next_state\n",
    "\n",
    "# Plot action distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(env.action_size), action_counts)\n",
    "plt.xlabel('Action', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Action Selection Frequency (50 Test Episodes)', fontsize=14)\n",
    "plt.xticks(range(env.action_size), \n",
    "          [env.actions[a] for a in range(env.action_size)], \n",
    "          rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Color bars\n",
    "colors = plt.cm.viridis(action_counts / action_counts.max())\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAction frequencies:\")\n",
    "for action_id, count in enumerate(action_counts):\n",
    "    percentage = count / action_counts.sum() * 100\n",
    "    print(f\"{env.actions[action_id]:20s}: {count:4.0f} ({percentage:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for models\n",
    "model_dir = Path('ai_training/controller/models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save PyTorch checkpoint\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "checkpoint_path = model_dir / f'controller_{timestamp}.pth'\n",
    "\n",
    "checkpoint = {\n",
    "    'episode': controller.episode,\n",
    "    'total_steps': controller.total_steps,\n",
    "    'policy_net_state': controller.policy_net.state_dict(),\n",
    "    'target_net_state': controller.target_net.state_dict(),\n",
    "    'optimizer_state': controller.optimizer.state_dict(),\n",
    "    'epsilon': controller.epsilon,\n",
    "    'config': controller.config,\n",
    "    'training_rewards': controller.training_rewards,\n",
    "    'training_losses': controller.training_losses,\n",
    "    'state_size': controller.state_size,\n",
    "    'action_size': controller.action_size\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"✓ Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "# Export weights as JSON for JavaScript\n",
    "weights_export = {}\n",
    "for name, param in controller.policy_net.state_dict().items():\n",
    "    weights_export[name] = param.cpu().numpy().tolist()\n",
    "\n",
    "export_data = {\n",
    "    'weights': weights_export,\n",
    "    'config': controller.config,\n",
    "    'state_size': controller.state_size,\n",
    "    'action_size': controller.action_size,\n",
    "    'actions': env.actions,\n",
    "    'training_stats': {\n",
    "        'episodes': controller.episode,\n",
    "        'total_steps': controller.total_steps,\n",
    "        'final_epsilon': controller.epsilon,\n",
    "        'avg_reward_last_20': float(np.mean(controller.training_rewards[-20:])),\n",
    "        'test_success_rate': test_results['success_rate']\n",
    "    }\n",
    "}\n",
    "\n",
    "json_path = model_dir / f'controller_weights_{timestamp}.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"✓ Weights exported: {json_path}\")\n",
    "\n",
    "# Save latest version\n",
    "latest_path = model_dir / 'controller_latest.pth'\n",
    "torch.save(checkpoint, latest_path)\n",
    "print(f\"✓ Latest checkpoint: {latest_path}\")\n",
    "\n",
    "latest_json = model_dir / 'controller_latest.json'\n",
    "with open(latest_json, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "print(f\"✓ Latest weights: {latest_json}\")\n",
    "\n",
    "print(\"\\n=== Model Export Complete ===\")\n",
    "print(f\"Files saved in: {model_dir.absolute()}\")\n",
    "print(f\"\\\\nTo use in JavaScript:\")\n",
    "print(f\"  1. Load weights from: {latest_json}\")\n",
    "print(f\"  2. Import AIController from public/AIController.js\")\n",
    "print(f\"  3. Initialize with loaded weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb72a3ca",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Integration with Three.js World\n",
    "1. Load [public/AIController.js](public/AIController.js) in your game\n",
    "2. Register agents using `controller.registerAgent()`\n",
    "3. Call `controller.start()` to begin autonomous behavior\n",
    "\n",
    "### Training Improvements\n",
    "- Collect real gameplay data from the 3D world\n",
    "- Fine-tune with actual agent interactions\n",
    "- Run [ai_controller_trainer.py](ai_controller_trainer.py) for advanced training\n",
    "\n",
    "### Further Development\n",
    "- Add more complex behaviors (cooperation, communication)\n",
    "- Implement multi-agent scenarios\n",
    "- Integrate with LLM for natural language reasoning\n",
    "- Add memory and learning from past experiences\n",
    "\n",
    "Run this notebook again anytime to retrain and improve the AI controller!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b26d5b8",
   "metadata": {},
   "source": [
    "## 9. Save and Export Model\n",
    "\n",
    "Save the trained controller for deployment in the web application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b81044c",
   "metadata": {},
   "source": [
    "## 8. Analyze Decision Making\n",
    "\n",
    "Visualize Q-values and action preferences to understand the agent's decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e44b27",
   "metadata": {},
   "source": [
    "## 7. Visualize Performance\n",
    "\n",
    "Create comprehensive visualizations of the AI controller's learning progress and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f82ea",
   "metadata": {},
   "source": [
    "## 6. Test the AI Controller\n",
    "\n",
    "Evaluate the trained controller's performance on test scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7442d",
   "metadata": {},
   "source": [
    "## 5. Train the AI Controller\n",
    "\n",
    "Train the agent for multiple episodes and observe learning progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06de85",
   "metadata": {},
   "source": [
    "## 4. Implement Decision Making Logic\n",
    "\n",
    "Implement the training loop that allows the agent to learn from experience through trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da10e16a",
   "metadata": {},
   "source": [
    "## 3. Create the AI Controller Class\n",
    "\n",
    "Define the Deep Q-Network (DQN) architecture and experience replay buffer for the AI controller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13839bc2",
   "metadata": {},
   "source": [
    "## 2. Define the Environment\n",
    "\n",
    "Set up the simulation environment where the AI agent will operate. This includes state space (agent observations) and action space (possible actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ce4b9",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import necessary libraries for reinforcement learning, neural networks, and data processing."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
