{"name":"web-llm-chat","version":"1.8.0","description":"Modern LLM chat with local Ollama and OpenAI support","main":"server.js","private":true,"type":"module","scripts":{"dev":"node server.js","start":"node server.js","test":"node --test tests/*.test.js","check:js":"node --check server.js && node --check public/app.js && node --check public/game.js","smoke:server":"node -e \"fetch('http://localhost:3000/health').then(r=>r.ok?r.json():Promise.reject(new Error('HTTP '+r.status))).then(j=>{console.log('OK /health', j);}).catch(e=>{console.error('FAIL /health', e.message);process.exit(1);})\"","smoke:ollama":"node -e \"fetch('http://localhost:11434/api/tags').then(r=>r.ok?r.json():Promise.reject(new Error('HTTP '+r.status))).then(j=>{console.log('OK ollama /api/tags');console.log('models', (j.models||[]).slice(0,5).map(m=>m.name).join(', '));}).catch(e=>{console.error('FAIL ollama /api/tags', e.message);process.exit(1);})\"","smoke":"npm run check:js && npm run smoke:server && npm run smoke:ollama"},"keywords":["llm","ollama","openai","chat","streaming","sse"],"author":"","license":"MIT","dependencies":{"@iarna/toml":"^3.0.0","bcryptjs":"^3.0.3","dotenv":"^16.4.5","express":"^4.19.2","jsonwebtoken":"^9.0.3","morgan":"^1.10.0","node-fetch":"^3.3.2","toml":"^3.0.0"},"devDependencies":{"supertest":"^7.1.4"},"engines":{"node":">=18.0.0"}}